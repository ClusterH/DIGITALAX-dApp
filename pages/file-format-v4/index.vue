<template>
  <div class="white-layout">
    <br><br><br>

    <h1 class="heading">
      DASH File Format Specification and File <br>Intercommunication Architecture. Revision 1.6.
    </h1>
    <br>

    <h3 class="subtitle">
      Quick Summary
    </h3>
    In this revision of DASH we continue to concentrate the focus further on the Transformation Set logic of the wrapper constructor. The Transformation Set feeds the input information through a 10 dimensional mathematical space in order to better identify its considerate center of mass. This can be likened to a series of edited match cuts— where each transition is interlinked only by relevant retained subject matter, beginning in a purposeful ‘zoomed in’ state, and so causing the experience of space and time to remain subjective.
    <br><br>
    Here, we are required to invert the whole current approach to information transfer through understanding one simple fact; you can’t force something to change. But, you can, however, change just about everything else around it. And usually, that’s enough. In other words, rather than attempting to pressure the information into a predetermined data structure, and so cause unavoidable inaccuracies and fidelity loss, we work to first filter and refine our understanding of the dimensions that the information sits within, and so ensure well informed and opinionated judgements are made.
    <br><br>
    We establish completeness of our model through opening the instruction set to unbiased upgrades and iterations over time, inserting the deliberate practise of being able to obtain immediate informative feedback and knowledge of our results. We are taking the approach of stepping back and seeing the bigger picture with flexibility to invent new options, rather than just picking the best available. Although this may seem like in practise, to add more information to the set, it actually works to reduce the information entropy overtime, making the information itself more useful and redundant i.e. less vulnerable to information loss and uncertainty. Thus, instead of just avoiding our inevitable inaccuracies or omissions, we are setting in motion the plans to fix them.
    <br><br><br><br>
    <div class="quotation">
      Sakichi Toyoda, the founder of the Toyota car company, developed a technique called “Five Why’s” for handling this. For example, sometimes a car would come off the Toyota production line and not start. Why? Well, imagine it was because the alternator belt had come loose. Most car companies would stop here and just fix the alternator belt. But Toyoda understood that was dodging the mistake — it would just lead it to come back again and again. So he insisted they keep asking “Why?”.
      <br><br>
      Why was the alternator belt loose? Because it hadn’t been put on correctly. Why? Because the person putting it on didn’t double-check to see if it had fit in correctly. Why? Because he was in too much of a hurry. Why? Because he had to walk all the way to the other side of the line to get the belts and by the time he got back he didn’t have enough time to double-check.
      <br><br>
      Aha! There, on the fifth why, we find the real cause of the mistake. And the solution is easy: move the box of alternator belts closer. But if we’d stopped at any earlier point (say, by just yelling at the alternator belt guy to always remember to double-check), we wouldn’t have actually fixed the problem. The same mistake would have happened again and again. Only by digging all the way to the root cause did we realize we needed to move the box of belts. The mistake pointed the way to the solution.
      <br><br>
      <div class="writer">
        -  Aaron Swartz, Raw Nerve
      </div>
    </div>
    <br><br><br><br>
    <h3 class="subtitle">
      Transformation Set Exploration
    </h3>
    We begin the Transformation set’s logic with a pre-vetting and pre-check of the information provided. The Goertzel Algorithm provides both a numerically efficient evaluation of the information at hand, where stability of the filter is guaranteed, due to the the z-domain pole placement on the z-plane's unit circle, and also a limit on the precision, ensuring that necessary conclusions can still be drawn about the information’s state at the initial stage, without the need for over exaggerated arithmetic.
    <br><br>
    Next, we incorporate the Fast Fourier Transform, inspecting the information within the frequency domain and so decomposing complicity of the source 3D file for the purpose of removing any perceived redundancy and ensuring efficient retention of only what remains relevant to the application environment. A Fast Fourier Transform (FFT) is performed across the texture maps and their segmented regions (Which were identified during the pre-processing stage) in order to distinguish between the high and low frequencies. Opinionated logic from the wrapper constructor instruction set, which is informed by the application environment from the game engine plugin, gives judicious choice as to which frequencies are removed, retained and reduced so that minimal distortion on the maps is incurred, despite the fact that much of the underlying information is discarded. The retained information is then converted back into the spatial domain and stored in a set of nested arrays, with each macro array component referencing a specific n cell region.
    <br><br>
    Voronoi tessellation succeeds the implementation of FFTs, where here we employ Fortune’s Plane Sweep Algorithm for generating the Voronoi diagram across the 3D model’s surface and defining the outward boundaries of the tessellated regions. The algorithm itself is iterative in nature, utilising a priority queue for listing future potential events that could act to change the beach line’s structure and so ensuring that consistent and repeated updates can be made to the data structure, as the sweep line moves in space, finding the changes the event causes in the beach line. Here, the final output includes the new optimised polycount and mesh density for the model, which is specific and considerate for the output environment itself.
    <br><br>
    Inclusion of the Louvain method is used to optimise the modularity already imposed from Voronoi tessellation and the Plane Sweep algorithm, by scoring across the detected communities. Louvain employs an iterative process for improving the scalability and detection of the modularity of a partition within a network/scope. All nodes or n cell regions are randomly ordered in the network, and then one by one are removed and inserted across a different community until no significant increase in modularity (input parameter) is verified. From here, the second stage involves community aggregation where all nodes belonging to the same community are merged into a single giant node. Thus, by clustering communities of communities after the first pass, it inherently considers the existence of a hierarchical organization in the network and so enables a more definitive ability to identify champion nodes, data and information in later dimensional passes.
    <br><br>
    Abduction is reasoning toward the “best” explanations for a set of encountered observations— a methodology for deriving root causes. Our reasoning is actively informed through its practice within our 10 dimensional transforms as a verification and inference technique, error diagnosis and re-check in relation to the deductions made thus far throughout the instruction stages of the Transformation Set. Through observing the data at hand, and using selected knowledge provided by the game engine plugin, inference rules are encoded that are able to assess causal relations between the relevant output application environment and the newly constructed vertex and appropriated region coordinate data. This further provides a more technically suited lens that characterises the overall ‘quality’ of the current assumptions of the success of the information when deployed in the application environment.
    <br><br>
    During the next stage, the problem size is reduced through the use of an efficient space–time reduced order model. This explicitly works to   accelerate the solution process without loss in accuracy as the solution space is searched for the least-cost path. Here we use the DCCR algorithm, which takes the same greedy strategy as in Dijkstra’s algorithm, but uses a non-linear weight function in searching for the best solution. Within DCCR, an improvement is accounted for through applying Chong’s k-shortest paths algorithm, which records k shortest paths, listed in increasing weight order, for each node. This interjection within the DCCR algorithm increases the chances of finding an optimal feasible path, through increasing the candidate paths to every node— a feature lacking from the non-linear weight function’s optimal-substructure property. Also, here we prelude the DCCR algorithm with the inclusion of the HZ 1 algorithm, proposed by Handler and Zang, to scope an either tighter cost bound in our solution space search, and so take into further consideration not just the identified paths, but also their attributed quality.
    <br><br>
    We further our knowledge of the information being manipulated by the instruction set through computing its associated derivation graph and the optimisation of the function over the sinks of the directed acyclic graph. By applying restructuring transformations to the information we are able to base our assumptions on the fact that the best solution is built out of the optimal solutions to the various subproblems. Here, we derive the champion scores/nodes of the network, making a last opinionated judgment as to which information is most fitted for output into the application environment and tailoring the interplay between entropy and redundancy depending on this environment’s properties.
    <br><br>
    The final dimensional pass involves the use of Monte Carlo Tree Search algorithm for ending the directive with an evaluation and potential restart, for retrieval of the best known information output candidate. This gives us the highest human relative fidelity for commercial application transfer and also openly injects an open source shibboleth into the very fabric of DASH, as we continue to scale the exposure of the solution’s instructions to positive Black Swan events— through wider community leverage.
    <br><br><br><br>
    <h3 class="subtitle">
      Transformation Set Architecture
    </h3>
    <img src="/images/code.png" al="" class="code1">
  </div>
</template>
<script>
export default {
  name: 'FileFormatPageV4'
}
</script>
<style lang="scss">
.white-layout {
  background: white;
  width: 100%;
  padding-left: 10%;
  padding-right: 10%;
  padding-top: 55px;
  margin-top: -55px;
  padding-bottom: 200px;

  font-family: "Courier New";
  font-weight: normal;
  font-size: 11pt;

  .heading {
    font-weight: bold;
    font-size: 15pt;
  }

  .subtitle {
    font-weight: bold;
    font-size: 12pt;
  }

  .quotation {
    padding-left: 5%;
    padding-right: 5%;
    text-align: left;
    font-style: italic;

    .writer {
      text-align: right;
    }
  }

  .note-list {
    margin-left: 15px;
  }

  .code1{
    margin-left: 0;
    margin-top: 40px;
    width: 800px;

    @media (max-width: 780px) {
      width: 100%;
      margin-left: 0;
      margin-right: 0;
    }
  }
}
</style>
